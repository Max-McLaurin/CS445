{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6410f41",
   "metadata": {},
   "source": [
    "# Deep Learning for Vision-Based SLAM: Comparing a MonoDepth2 Baseline to a Full SLAM Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55094561",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Our project aims to explore and quantify the benefits of integrating deep learning components into a full monocular visual SLAM (Simultaneous Localization and Mapping) system. We plan to implement a baseline depth estimation model inspired by MonoDepth2 and compare its performance to a full SLAM pipeline that additionally estimates camera pose, performs loop closure detection, and builds a global 3D map. \n",
    "\n",
    "Given the potential complexity of this task, we have also considered an alternative approach to quantify the benefits of deep learning in SLAM — compare deep learning to traditional methods for monocular visual odometry. This would involve implementing a traditional SIFT-based pose estimation for visual odometry and compare this performance to using a deep learning model resembling DeepVO. Time permitting, we could integrate both these pose estimation steps into respective SLAM systems and compare the results.\n",
    "\n",
    "This project will not only deepen our understanding of deep learning applications in robotics but also provide insights into how spatial and temporal context can improve depth estimation and localization. Both of us had a deep interest in doing something with computer vision for our final project. As this was a shared interest we wanted to challenge ourselves and do something beyond image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7f6c25",
   "metadata": {},
   "source": [
    "## Research Questions and Hypotheses\n",
    "**Potential Questions:**\n",
    "- How does the integration of pose estimation and loop closure in a full SLAM pipeline improve the accuracy of depth estimation compared to a baseline MonoDepth2 model?\n",
    "- What are the differences in performance when evaluating per-frame depth accuracy, overall camera trajectory (pose estimation), and global map consistency?\n",
    "- How does Monocular Visual Odometry using Deep Learning (like DeepVO) compare to a more traditional feature-based approach with SIFT?\n",
    "\n",
    "**Hypotheses:**\n",
    "- The full SLAM system will produce more accurate depth maps (lower RMSE and absolute relative error) than MonoDepth2 by leveraging multi-frame information.\n",
    "- The SLAM system will demonstrate significantly improved camera localization (measured by Absolute Trajectory Error and Relative Pose Error) compared to using a single-frame depth estimation model.\n",
    "- The global 3D map generated by the SLAM pipeline will exhibit higher consistency and reduced drift compared to the baseline depth predictions from MonoDepth2.\n",
    "- DeepVO will produce more flexible and robust relative trajectory estimations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6844c0d",
   "metadata": {},
   "source": [
    "## Methods\n",
    "**Baseline Model – MonoDepth2:**\n",
    "- Implement an existing MonoDepth2 network using PyTorch to predict per-frame dense depth maps.\n",
    "- Train the model on publicly available datasets (KITTI) and evaluate using standard depth metrics such as RMSE, Absolute Relative Error, and threshold accuracy.\n",
    "\n",
    "**Full SLAM Pipeline:**\n",
    "- **Pose Estimation Module:** Develop a CNN+LSTM-based visual odometry model to estimate relative camera motion between frames.\n",
    "- **Loop Closure Detection:** Integrate a deep learning module to detect revisited areas and trigger map optimization. Or suppliment with a premade one such as NetVLAD\n",
    "- **Mapping Module:** Fuse per-frame depth predictions with pose estimates to build a global 3D map. Apply bundle adjustment techniques to refine both the camera trajectory and the map.\n",
    "\n",
    "**Evaluation:**\n",
    "- Compare per-frame depth outputs from both systems using standard metrics.\n",
    "- Evaluate SLAM’s pose accuracy using Absolute Trajectory Error (ATE) and Relative Pose Error (RPE).\n",
    "- Assess global map quality via qualitative visualization and cloud alignment metrics.\n",
    "\n",
    "**Data & Code Sources:**\n",
    "- Use datasets like KITTI or TUM RGB-D.\n",
    "- Use or adapt open-source code for MonoDepth2, visual odometry, and loop closure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f297b2",
   "metadata": {},
   "source": [
    "## Division of Work\n",
    "- **Trevor Chartier:**\n",
    "  - Set up project repository and environment.\n",
    "  - Lead MonoDepth2 baseline implementation.\n",
    "  - Develop visual odometry modules.\n",
    "  - Implement pose evaluation metrics.\n",
    "\n",
    "- **Max McLaurin:**\n",
    "  - Implement loop closure and mapping modules.\n",
    "  - Integrate global map construction.\n",
    "  - Integrate all modules and handle synchronization.\n",
    "  - Manage experiment comparisons and documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93323e9c",
   "metadata": {},
   "source": [
    "## Possible Results\n",
    "- The full SLAM pipeline is expected to produce lower depth errors compared to MonoDepth2 through the use of additional pose and temporal information.\n",
    "- SLAM should demonstrate better camera trajectory accuracy (lower ATE and RPE).\n",
    "- The 3D map from SLAM will be more coherent and less prone to drift."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139e712a",
   "metadata": {},
   "source": [
    "## Timeline\n",
    "**Sprint1 1 (Setup & Data Preparation):**\n",
    "   - Project setup, data download, and preprocessing.\n",
    "\n",
    "**Sprint 2 (Baseline Implementation):**\n",
    "   - MonoDepth2 model training and evaluation.\n",
    "\n",
    "**Sprint 3 (SLAM Module Development):**\n",
    "   - Visual odometry and loop closure integration.\n",
    "\n",
    "**Sprint 4 (Integration & Evaluation):**\n",
    "   - Final integration, testing, evaluation, and result visualization."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
